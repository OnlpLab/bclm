{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dan/bclm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf = pd.read_csv('../data/spdf_fixed.csv.gz', low_memory=False)\n",
    "spdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "DEFAULT_FIELDS = ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "def sentence_to_conllu_token_list(sent, add_misc=['ner_escaped', 'token_id', 'token_str', 'biose_layer0', \n",
    "                                                  'biose_layer1', 'biose_layer2', 'biose_layer3'],\n",
    "                                  metadata=['sent_id', 'global_sent_id', 'set', 'duplicate_sent_id', 'very_similar_sent_id']):\n",
    "    metadata = OrderedDict([(m, str(sent[m].iat[0])) for m in metadata])\n",
    "    \n",
    "    if add_misc is not None:\n",
    "        amisc = sent.apply(lambda x: [m+'='+str(x[m]) for m in add_misc], axis=1)\n",
    "        amisc = amisc.str.join('|')\n",
    "    token_morphs = (sent.groupby('token_id').id.agg([min, max])\n",
    "                 .pipe(lambda x: x[x['max'] - x['min']>0])\n",
    "                 .assign(token_morphs=lambda x: x['min'].astype(str)+'-'+x['max'].astype(str))\n",
    "                 .set_index('min')['token_morphs'])   \n",
    "    token_list = []\n",
    "    \n",
    "    for (i, row), token_str, am in zip(sent[DEFAULT_FIELDS].iterrows(), sent['token_str'].tolist(), amisc.tolist()):\n",
    "        token = OrderedDict(row)\n",
    "        if type(token['feats']) == str and token['feats']!='_':\n",
    "            #token['feats'] = OrderedDict([f.split('=') for f in token['feats'].split('|')])\n",
    "            feats = OrderedDict()\n",
    "            for f in token['feats'].split('|'):\n",
    "                k,v = f.split('=')\n",
    "                if k not in feats:\n",
    "                    feats[k] = v\n",
    "                else:\n",
    "                    feats[k] = feats[k]+','+v\n",
    "            token['feats'] = feats\n",
    "                        \n",
    "        am = OrderedDict([f.split('=') for f in am.split('|') ])\n",
    "        if token['misc'] =='_' or token['misc'] is None:\n",
    "            token['misc'] = am\n",
    "        elif type(token['misc'])==str:\n",
    "            token['misc'] = OrderedDict([('MISC', token['misc'])])\n",
    "            token['misc'].update(am)\n",
    "        elif type(token['misc'])==OrderedDict:\n",
    "            am.update(token['misc'])\n",
    "            token['misc'] = am\n",
    "            \n",
    "        if token['id'] in token_morphs:\n",
    "            tok_row = OrderedDict([(f, '_') for f in DEFAULT_FIELDS])\n",
    "            id_tup = token_morphs[token['id']].split('-')\n",
    "            tok_row['id'] = (id_tup[0], '-', id_tup[1])\n",
    "            tok_row['form'] = token_str\n",
    "            token_list.append(tok_row)\n",
    "            \n",
    "        token_list.append(token)\n",
    "        \n",
    "    return token_list, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu.models import TokenList\n",
    "sp_conllu_sents = [TokenList(*sentence_to_conllu_token_list(spdf[spdf.sent_id==i])) for i in spdf.sent_id.drop_duplicates().tolist()]\n",
    "print(sp_conllu_sents[2].serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sp_conllu_sents:\n",
    "    for t in s:\n",
    "        if type(t['feats'])==OrderedDict:\n",
    "            for k, v in t['feats'].items():\n",
    "                if ',' in v:\n",
    "                    print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
